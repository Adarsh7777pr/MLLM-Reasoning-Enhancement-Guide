# 📊 推理能力评估基准整理（Benchmarks for Reasoning in MLLMs）

以下是截至 2025 年 4 月，针对多模态大语言模型（MLLM）推理能力评估的代表性 benchmark 数据集与测试平台，涵盖逻辑一致性、图像序列、多模态对比推理、图表分析等维度。

---

## 🔍 综合推理评估 Benchmark

### 1. [EMMA](https://arxiv.org/abs/2501.05444) — Enhanced MultiModal ReAsoning Benchmark
- **模态**：图像 + 文本 + 数学 + 编程
- **亮点**：多学科高复杂度推理，覆盖数学、物理、代码等跨模态任务，强调信息整合与结构化输出。

### 2. [MMMU-Pro](https://arxiv.org/abs/2409.02813)
- **模态**：图像 + 文本（OCR）
- **亮点**：增强版本的 MMMU，提供更高难度、多学科多模态 QA 测试（医学、工程等）

### 3. [OlympiadBench](https://arxiv.org/abs/2402.14008)
- **模态**：图像 + 数学文本（中英双语）
- **亮点**：类奥数风格任务，兼顾语言与视觉信息理解，评估 AI 的学科通才能力

---

## 🖼️ 视觉与图像推理 Benchmark

### 4. [Mementos](https://aclanthology.org/2024.acl-long.25/)
- **模态**：图像序列 + 问答
- **亮点**：强调多步推理与动态视觉信息建模，适用于视频图像处理任务评估

### 5. [MLLM-CompBench](https://arxiv.org/abs/2407.16837)
- **模态**：图像对 + 指令文本
- **亮点**：细粒度比较推理能力测试（存在性、数量、空间关系、情绪等8个维度）

### 6. [LogicVista](https://arxiv.org/abs/2407.04973)
- **模态**：视觉场景 + 多项选择逻辑问题
- **亮点**：测试模型是否具备跨场景逻辑一致性与形式化推理能力

---

## 📈 图表与结构化数据推理 Benchmark

### 7. [ChartBench](https://arxiv.org/abs/2312.15915)
- **模态**：图表 + 表格 + 问答指令
- **亮点**：评估模型对图表信息（折线图、饼图等）的理解、提取与推理生成能力

---

## 📚 使用建议

- ✅ 可用于 finetune 后效果验证 / zero-shot 推理测评
- 🔍 建议结合 CoT 输出格式统一分析
- 🧪 可与多维指标（准确率、逻辑一致性、思维链完备性）结合

如有新的 benchmark 推荐，欢迎通过 PR 更新本页 🙌

